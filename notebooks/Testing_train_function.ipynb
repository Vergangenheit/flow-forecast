{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import wandb\n",
    "import pycountry\n",
    "from flood_forecast.time_model import PyTorchForecast\n",
    "from flood_forecast.trainer import train_function\n",
    "from wandb.wandb_run import Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>AT</th>\n",
       "      <th>BE</th>\n",
       "      <th>BG</th>\n",
       "      <th>CH</th>\n",
       "      <th>CZ</th>\n",
       "      <th>DE</th>\n",
       "      <th>DK</th>\n",
       "      <th>EE</th>\n",
       "      <th>ES</th>\n",
       "      <th>...</th>\n",
       "      <th>LV</th>\n",
       "      <th>NL</th>\n",
       "      <th>NO</th>\n",
       "      <th>PL</th>\n",
       "      <th>PT</th>\n",
       "      <th>RO</th>\n",
       "      <th>SI</th>\n",
       "      <th>SK</th>\n",
       "      <th>SE</th>\n",
       "      <th>UK</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1986-01-01</th>\n",
       "      <td>1986-01-01</td>\n",
       "      <td>0.047786</td>\n",
       "      <td>0.023020</td>\n",
       "      <td>0.048940</td>\n",
       "      <td>0.065907</td>\n",
       "      <td>0.041685</td>\n",
       "      <td>0.031583</td>\n",
       "      <td>0.017365</td>\n",
       "      <td>0.014149</td>\n",
       "      <td>0.079043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019004</td>\n",
       "      <td>0.014293</td>\n",
       "      <td>0.010351</td>\n",
       "      <td>0.029919</td>\n",
       "      <td>0.076675</td>\n",
       "      <td>0.029107</td>\n",
       "      <td>0.015193</td>\n",
       "      <td>0.054001</td>\n",
       "      <td>0.017463</td>\n",
       "      <td>0.030419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-01-02</th>\n",
       "      <td>1986-01-02</td>\n",
       "      <td>0.045921</td>\n",
       "      <td>0.036297</td>\n",
       "      <td>0.067995</td>\n",
       "      <td>0.077502</td>\n",
       "      <td>0.026427</td>\n",
       "      <td>0.023506</td>\n",
       "      <td>0.014981</td>\n",
       "      <td>0.015682</td>\n",
       "      <td>0.119019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013771</td>\n",
       "      <td>0.020373</td>\n",
       "      <td>0.006469</td>\n",
       "      <td>0.031359</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.044379</td>\n",
       "      <td>0.024623</td>\n",
       "      <td>0.034362</td>\n",
       "      <td>0.008086</td>\n",
       "      <td>0.022146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-01-03</th>\n",
       "      <td>1986-01-03</td>\n",
       "      <td>0.067308</td>\n",
       "      <td>0.021352</td>\n",
       "      <td>0.101287</td>\n",
       "      <td>0.103680</td>\n",
       "      <td>0.057274</td>\n",
       "      <td>0.046181</td>\n",
       "      <td>0.023478</td>\n",
       "      <td>0.009570</td>\n",
       "      <td>0.106574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011871</td>\n",
       "      <td>0.010782</td>\n",
       "      <td>0.007217</td>\n",
       "      <td>0.027554</td>\n",
       "      <td>0.160308</td>\n",
       "      <td>0.047235</td>\n",
       "      <td>0.032093</td>\n",
       "      <td>0.023788</td>\n",
       "      <td>0.010004</td>\n",
       "      <td>0.060345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-01-04</th>\n",
       "      <td>1986-01-04</td>\n",
       "      <td>0.043833</td>\n",
       "      <td>0.050756</td>\n",
       "      <td>0.039337</td>\n",
       "      <td>0.075418</td>\n",
       "      <td>0.025843</td>\n",
       "      <td>0.025011</td>\n",
       "      <td>0.020003</td>\n",
       "      <td>0.008595</td>\n",
       "      <td>0.135060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013604</td>\n",
       "      <td>0.030366</td>\n",
       "      <td>0.007998</td>\n",
       "      <td>0.025986</td>\n",
       "      <td>0.208236</td>\n",
       "      <td>0.037510</td>\n",
       "      <td>0.028663</td>\n",
       "      <td>0.018115</td>\n",
       "      <td>0.009546</td>\n",
       "      <td>0.030981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-01-05</th>\n",
       "      <td>1986-01-05</td>\n",
       "      <td>0.082394</td>\n",
       "      <td>0.014302</td>\n",
       "      <td>0.033055</td>\n",
       "      <td>0.090867</td>\n",
       "      <td>0.065186</td>\n",
       "      <td>0.028168</td>\n",
       "      <td>0.016261</td>\n",
       "      <td>0.009780</td>\n",
       "      <td>0.095232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013913</td>\n",
       "      <td>0.012728</td>\n",
       "      <td>0.007241</td>\n",
       "      <td>0.047764</td>\n",
       "      <td>0.115451</td>\n",
       "      <td>0.037254</td>\n",
       "      <td>0.057101</td>\n",
       "      <td>0.072843</td>\n",
       "      <td>0.013872</td>\n",
       "      <td>0.023346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time        AT        BE        BG        CH        CZ  \\\n",
       "datetime                                                                  \n",
       "1986-01-01 1986-01-01  0.047786  0.023020  0.048940  0.065907  0.041685   \n",
       "1986-01-02 1986-01-02  0.045921  0.036297  0.067995  0.077502  0.026427   \n",
       "1986-01-03 1986-01-03  0.067308  0.021352  0.101287  0.103680  0.057274   \n",
       "1986-01-04 1986-01-04  0.043833  0.050756  0.039337  0.075418  0.025843   \n",
       "1986-01-05 1986-01-05  0.082394  0.014302  0.033055  0.090867  0.065186   \n",
       "\n",
       "                  DE        DK        EE        ES  ...        LV        NL  \\\n",
       "datetime                                            ...                       \n",
       "1986-01-01  0.031583  0.017365  0.014149  0.079043  ...  0.019004  0.014293   \n",
       "1986-01-02  0.023506  0.014981  0.015682  0.119019  ...  0.013771  0.020373   \n",
       "1986-01-03  0.046181  0.023478  0.009570  0.106574  ...  0.011871  0.010782   \n",
       "1986-01-04  0.025011  0.020003  0.008595  0.135060  ...  0.013604  0.030366   \n",
       "1986-01-05  0.028168  0.016261  0.009780  0.095232  ...  0.013913  0.012728   \n",
       "\n",
       "                  NO        PL        PT        RO        SI        SK  \\\n",
       "datetime                                                                 \n",
       "1986-01-01  0.010351  0.029919  0.076675  0.029107  0.015193  0.054001   \n",
       "1986-01-02  0.006469  0.031359  0.106900  0.044379  0.024623  0.034362   \n",
       "1986-01-03  0.007217  0.027554  0.160308  0.047235  0.032093  0.023788   \n",
       "1986-01-04  0.007998  0.025986  0.208236  0.037510  0.028663  0.018115   \n",
       "1986-01-05  0.007241  0.047764  0.115451  0.037254  0.057101  0.072843   \n",
       "\n",
       "                  SE        UK  \n",
       "datetime                        \n",
       "1986-01-01  0.017463  0.030419  \n",
       "1986-01-02  0.008086  0.022146  \n",
       "1986-01-03  0.010004  0.060345  \n",
       "1986-01-04  0.009546  0.030981  \n",
       "1986-01-05  0.013872  0.023346  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wind: DataFrame = pd.read_csv('../data/wind.csv')\n",
    "wind['datetime']: Series = pd.to_datetime(wind['time']).dt.date\n",
    "wind.set_index('datetime', drop=True, inplace=True)\n",
    "wind['time'] = wind['time'].astype('datetime64[s]')\n",
    "wind.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time\n",
      "EL\n",
      "UK\n"
     ]
    }
   ],
   "source": [
    "names: Dict = {}\n",
    "for code in wind.columns:\n",
    "    try:\n",
    "        names[code] = pycountry.countries.get(alpha_2=code).name\n",
    "    except:\n",
    "        print(code)\n",
    "\n",
    "# For some reason, these two were not present\n",
    "names['EL'] = 'Greece'\n",
    "names['UK'] = 'United Kingdom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind.rename(columns = names, inplace=True)\n",
    "# wind['year'] = pd.to_datetime(wind['time']).map(lambda x: x.year)\n",
    "wind['month'] = pd.to_datetime(wind['time']).map(lambda x: x.month)\n",
    "wind['weekday'] = pd.to_datetime(wind['time']).map(lambda x: x.weekday())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind.to_csv('../data/wind_train.csv', index=True, index_label='datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config file for WanDB sweeps\n",
    "\n",
    "def make_config_file(file_path: str, df_len: int) -> Dict:\n",
    "    train_number: float = df_len * .7\n",
    "    validation_number: float = df_len *.9\n",
    "    config_default={\n",
    "      \"model_name\": \"DecoderTransformer\",\n",
    "      \"model_type\": \"PyTorch\",\n",
    "      \"takes_target\": False,\n",
    "      \"model_params\": {\n",
    "      \"n_time_series\":30,\n",
    "      \"n_head\": 8,\n",
    "      \"forecast_history\":90,\n",
    "      \"n_embd\": 1, \n",
    "      \"num_layer\": 5,\n",
    "      \"dropout\":0.1,\n",
    "      \"q_len\": 1,\n",
    "      \"scale_att\": False,\n",
    "      \"forecast_length\": 30, \n",
    "      \"additional_params\":{}\n",
    "     },\n",
    "     \"dataset_params\":\n",
    "     {\n",
    "         \"class\": \"default\",\n",
    "          \"training_path\": file_path,\n",
    "          \"validation_path\": file_path,\n",
    "          \"test_path\": file_path,\n",
    "          \"batch_size\":64,\n",
    "          \"forecast_history\":90,\n",
    "          \"forecast_length\":30,\n",
    "          \"train_end\": int(train_number),\n",
    "          \"valid_start\":int(train_number+1),\n",
    "          \"valid_end\": int(validation_number),\n",
    "          \"target_col\": ['Austria'],\n",
    "          \"relevant_cols\": ['Austria', 'Belgium', 'Bulgaria', 'Switzerland', 'Czechia',\n",
    "                            'Germany', 'Denmark', 'Estonia', 'Spain', 'Finland', 'France', 'Greece',\n",
    "                            'Croatia', 'Hungary', 'Ireland', 'Italy', 'Lithuania', 'Luxembourg',\n",
    "                            'Latvia', 'Netherlands', 'Norway', 'Poland', 'Portugal', 'Romania',\n",
    "                            'Slovenia', 'Slovakia', 'Sweden', 'United Kingdom', 'month', 'weekday'],\n",
    "          \"scaler\": \"StandardScaler\", \n",
    "          \"interpolate\": False,\n",
    "          \"sort_column\":\"time\",\n",
    "     },\n",
    "     \"training_params\":\n",
    "      {\n",
    "        \"criterion\":\"DilateLoss\",\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"optim_params\":\n",
    "        {\n",
    "        },\n",
    "        \"lr\": 0.001,\n",
    "        \"epochs\": 10,\n",
    "        \"batch_size\":64\n",
    "      },\n",
    "      \"early_stopping\": {\n",
    "          \"patience\":3\n",
    "      },\n",
    "      \"GCS\": False,\n",
    "      \"sweep\":False,\n",
    "      \"wandb\":False,\n",
    "      \"forward_params\":{},\n",
    "      \"metrics\":[\"DilateLoss\"],\n",
    "      \"inference_params\":\n",
    "        {     \n",
    "              \"datetime_start\":\"2010-01-01\",\n",
    "                \"hours_to_forecast\": 2000, \n",
    "                \"test_csv_path\":file_path,\n",
    "                \"decoder_params\":{\n",
    "                    \"decoder_function\": \"simple_decode\", \n",
    "                  \"unsqueeze_dim\": 1\n",
    "                },\n",
    "                \"dataset_params\":{\n",
    "                  \"file_path\": file_path,\n",
    "                  \"forecast_history\":90,\n",
    "                  \"forecast_length\":30,\n",
    "                  \"relevant_cols\": ['Austria', 'Belgium', 'Bulgaria', 'Switzerland', 'Czechia',\n",
    "                            'Germany', 'Denmark', 'Estonia', 'Spain', 'Finland', 'France', 'Greece',\n",
    "                            'Croatia', 'Hungary', 'Ireland', 'Italy', 'Lithuania', 'Luxembourg',\n",
    "                            'Latvia', 'Netherlands', 'Norway', 'Poland', 'Portugal', 'Romania',\n",
    "                            'Slovenia', 'Slovakia', 'Sweden', 'United Kingdom', 'month', 'weekday'],\n",
    "                  \"target_col\": ['Austria'],\n",
    "                  \"scaling\": \"StandardScaler\",\n",
    "                  \"interpolate_param\": False\n",
    "                }\n",
    "          },\n",
    "    }\n",
    "\n",
    "    return config_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path: str = '../data/wind_train.csv'\n",
    "full_len: int = len(pd.read_csv(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_file: Dict = make_config_file(file_path, full_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mloloheia\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.27<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">sage-shape-55</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/loloheia/pretrained-wind-updated\" target=\"_blank\">https://wandb.ai/loloheia/pretrained-wind-updated</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/loloheia/pretrained-wind-updated/runs/144e1whk\" target=\"_blank\">https://wandb.ai/loloheia/pretrained-wind-updated/runs/144e1whk</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\Lorenzo\\PycharmProjects\\flow-forecast\\notebooks\\wandb\\run-20210519_113751-144e1whk</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"pretrained-wind-updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interpolate should be below\n",
      "[]\n",
      "Now loading ../data/wind_train.csv\n",
      "scaling now\n",
      "interpolate should be below\n",
      "[]\n",
      "Now loading ../data/wind_train.csv\n",
      "scaling now\n",
      "interpolate should be below\n",
      "[]\n",
      "Now loading ../data/wind_train.csv\n",
      "scaling now\n",
      "Using Wandb config:\n",
      "{}\n",
      "Torch is using cuda\n",
      "running torch_single_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "Using non-full backward hooks on a Module that does not take as input a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The running loss is: \n",
      "746.5062863826752\n",
      "The number of items in train is: 118\n",
      "The loss for epoch 0\n",
      "6.326324460870128\n",
      "Computing validation loss\n",
      "running torch_single_train\n",
      "The running loss is: \n",
      "557.404262304306\n",
      "The number of items in train is: 118\n",
      "The loss for epoch 1\n",
      "4.723764934782254\n",
      "Computing validation loss\n",
      "running torch_single_train\n",
      "The running loss is: \n",
      "562.8397541046143\n",
      "The number of items in train is: 118\n",
      "The loss for epoch 2\n",
      "4.769828424615375\n",
      "Computing validation loss\n",
      "running torch_single_train\n",
      "The running loss is: \n",
      "539.7109055519104\n",
      "The number of items in train is: 118\n",
      "The loss for epoch 3\n",
      "4.573821233490766\n",
      "Computing validation loss\n",
      "running torch_single_train\n",
      "The running loss is: \n",
      "544.1412591934204\n",
      "The number of items in train is: 118\n",
      "The loss for epoch 4\n",
      "4.611366603334071\n",
      "Computing validation loss\n",
      "1\n",
      "running torch_single_train\n",
      "The running loss is: \n",
      "532.4913876056671\n",
      "The number of items in train is: 118\n",
      "The loss for epoch 5\n",
      "4.512638878014128\n",
      "Computing validation loss\n",
      "running torch_single_train\n",
      "The running loss is: \n",
      "519.6803777217865\n",
      "The number of items in train is: 118\n",
      "The loss for epoch 6\n",
      "4.4040709976422585\n",
      "Computing validation loss\n",
      "running torch_single_train\n",
      "The running loss is: \n",
      "541.6129760742188\n",
      "The number of items in train is: 118\n",
      "The loss for epoch 7\n",
      "4.5899404752052435\n",
      "Computing validation loss\n",
      "1\n",
      "running torch_single_train\n",
      "The running loss is: \n",
      "539.6312091350555\n",
      "The number of items in train is: 118\n",
      "The loss for epoch 8\n",
      "4.573145840127589\n",
      "Computing validation loss\n",
      "running torch_single_train\n",
      "The running loss is: \n",
      "513.5610547065735\n",
      "The number of items in train is: 118\n",
      "The loss for epoch 9\n",
      "4.3522123280218095\n",
      "Computing validation loss\n",
      "Computing validation loss\n"
     ]
    }
   ],
   "source": [
    "trained_model: PyTorchForecast = train_function(\"PyTorch\", make_config_file(file_path, full_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flood-forecast",
   "language": "python",
   "name": "flood-forecast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
