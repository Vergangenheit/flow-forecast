{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import wandb\n",
    "import pycountry\n",
    "from flood_forecast.time_model import PyTorchForecast\n",
    "from flood_forecast.trainer import train_function\n",
    "from wandb.wandb_run import Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>AT</th>\n",
       "      <th>BE</th>\n",
       "      <th>BG</th>\n",
       "      <th>CH</th>\n",
       "      <th>CZ</th>\n",
       "      <th>DE</th>\n",
       "      <th>DK</th>\n",
       "      <th>EE</th>\n",
       "      <th>ES</th>\n",
       "      <th>...</th>\n",
       "      <th>LV</th>\n",
       "      <th>NL</th>\n",
       "      <th>NO</th>\n",
       "      <th>PL</th>\n",
       "      <th>PT</th>\n",
       "      <th>RO</th>\n",
       "      <th>SI</th>\n",
       "      <th>SK</th>\n",
       "      <th>SE</th>\n",
       "      <th>UK</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1986-01-01</th>\n",
       "      <td>1986-01-01</td>\n",
       "      <td>0.047786</td>\n",
       "      <td>0.023020</td>\n",
       "      <td>0.048940</td>\n",
       "      <td>0.065907</td>\n",
       "      <td>0.041685</td>\n",
       "      <td>0.031583</td>\n",
       "      <td>0.017365</td>\n",
       "      <td>0.014149</td>\n",
       "      <td>0.079043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019004</td>\n",
       "      <td>0.014293</td>\n",
       "      <td>0.010351</td>\n",
       "      <td>0.029919</td>\n",
       "      <td>0.076675</td>\n",
       "      <td>0.029107</td>\n",
       "      <td>0.015193</td>\n",
       "      <td>0.054001</td>\n",
       "      <td>0.017463</td>\n",
       "      <td>0.030419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-01-02</th>\n",
       "      <td>1986-01-02</td>\n",
       "      <td>0.045921</td>\n",
       "      <td>0.036297</td>\n",
       "      <td>0.067995</td>\n",
       "      <td>0.077502</td>\n",
       "      <td>0.026427</td>\n",
       "      <td>0.023506</td>\n",
       "      <td>0.014981</td>\n",
       "      <td>0.015682</td>\n",
       "      <td>0.119019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013771</td>\n",
       "      <td>0.020373</td>\n",
       "      <td>0.006469</td>\n",
       "      <td>0.031359</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.044379</td>\n",
       "      <td>0.024623</td>\n",
       "      <td>0.034362</td>\n",
       "      <td>0.008086</td>\n",
       "      <td>0.022146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-01-03</th>\n",
       "      <td>1986-01-03</td>\n",
       "      <td>0.067308</td>\n",
       "      <td>0.021352</td>\n",
       "      <td>0.101287</td>\n",
       "      <td>0.103680</td>\n",
       "      <td>0.057274</td>\n",
       "      <td>0.046181</td>\n",
       "      <td>0.023478</td>\n",
       "      <td>0.009570</td>\n",
       "      <td>0.106574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011871</td>\n",
       "      <td>0.010782</td>\n",
       "      <td>0.007217</td>\n",
       "      <td>0.027554</td>\n",
       "      <td>0.160308</td>\n",
       "      <td>0.047235</td>\n",
       "      <td>0.032093</td>\n",
       "      <td>0.023788</td>\n",
       "      <td>0.010004</td>\n",
       "      <td>0.060345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-01-04</th>\n",
       "      <td>1986-01-04</td>\n",
       "      <td>0.043833</td>\n",
       "      <td>0.050756</td>\n",
       "      <td>0.039337</td>\n",
       "      <td>0.075418</td>\n",
       "      <td>0.025843</td>\n",
       "      <td>0.025011</td>\n",
       "      <td>0.020003</td>\n",
       "      <td>0.008595</td>\n",
       "      <td>0.135060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013604</td>\n",
       "      <td>0.030366</td>\n",
       "      <td>0.007998</td>\n",
       "      <td>0.025986</td>\n",
       "      <td>0.208236</td>\n",
       "      <td>0.037510</td>\n",
       "      <td>0.028663</td>\n",
       "      <td>0.018115</td>\n",
       "      <td>0.009546</td>\n",
       "      <td>0.030981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-01-05</th>\n",
       "      <td>1986-01-05</td>\n",
       "      <td>0.082394</td>\n",
       "      <td>0.014302</td>\n",
       "      <td>0.033055</td>\n",
       "      <td>0.090867</td>\n",
       "      <td>0.065186</td>\n",
       "      <td>0.028168</td>\n",
       "      <td>0.016261</td>\n",
       "      <td>0.009780</td>\n",
       "      <td>0.095232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013913</td>\n",
       "      <td>0.012728</td>\n",
       "      <td>0.007241</td>\n",
       "      <td>0.047764</td>\n",
       "      <td>0.115451</td>\n",
       "      <td>0.037254</td>\n",
       "      <td>0.057101</td>\n",
       "      <td>0.072843</td>\n",
       "      <td>0.013872</td>\n",
       "      <td>0.023346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time        AT        BE        BG        CH        CZ  \\\n",
       "datetime                                                                  \n",
       "1986-01-01 1986-01-01  0.047786  0.023020  0.048940  0.065907  0.041685   \n",
       "1986-01-02 1986-01-02  0.045921  0.036297  0.067995  0.077502  0.026427   \n",
       "1986-01-03 1986-01-03  0.067308  0.021352  0.101287  0.103680  0.057274   \n",
       "1986-01-04 1986-01-04  0.043833  0.050756  0.039337  0.075418  0.025843   \n",
       "1986-01-05 1986-01-05  0.082394  0.014302  0.033055  0.090867  0.065186   \n",
       "\n",
       "                  DE        DK        EE        ES  ...        LV        NL  \\\n",
       "datetime                                            ...                       \n",
       "1986-01-01  0.031583  0.017365  0.014149  0.079043  ...  0.019004  0.014293   \n",
       "1986-01-02  0.023506  0.014981  0.015682  0.119019  ...  0.013771  0.020373   \n",
       "1986-01-03  0.046181  0.023478  0.009570  0.106574  ...  0.011871  0.010782   \n",
       "1986-01-04  0.025011  0.020003  0.008595  0.135060  ...  0.013604  0.030366   \n",
       "1986-01-05  0.028168  0.016261  0.009780  0.095232  ...  0.013913  0.012728   \n",
       "\n",
       "                  NO        PL        PT        RO        SI        SK  \\\n",
       "datetime                                                                 \n",
       "1986-01-01  0.010351  0.029919  0.076675  0.029107  0.015193  0.054001   \n",
       "1986-01-02  0.006469  0.031359  0.106900  0.044379  0.024623  0.034362   \n",
       "1986-01-03  0.007217  0.027554  0.160308  0.047235  0.032093  0.023788   \n",
       "1986-01-04  0.007998  0.025986  0.208236  0.037510  0.028663  0.018115   \n",
       "1986-01-05  0.007241  0.047764  0.115451  0.037254  0.057101  0.072843   \n",
       "\n",
       "                  SE        UK  \n",
       "datetime                        \n",
       "1986-01-01  0.017463  0.030419  \n",
       "1986-01-02  0.008086  0.022146  \n",
       "1986-01-03  0.010004  0.060345  \n",
       "1986-01-04  0.009546  0.030981  \n",
       "1986-01-05  0.013872  0.023346  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wind: DataFrame = pd.read_csv('../data/wind.csv')\n",
    "wind['datetime']: Series = pd.to_datetime(wind['time']).dt.date\n",
    "wind.set_index('datetime', drop=True, inplace=True)\n",
    "wind['time'] = wind['time'].astype('datetime64[s]')\n",
    "wind.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time\n",
      "EL\n",
      "UK\n"
     ]
    }
   ],
   "source": [
    "names: Dict = {}\n",
    "for code in wind.columns:\n",
    "    try:\n",
    "        names[code] = pycountry.countries.get(alpha_2=code).name\n",
    "    except:\n",
    "        print(code)\n",
    "\n",
    "# For some reason, these two were not present\n",
    "names['EL'] = 'Greece'\n",
    "names['UK'] = 'United Kingdom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind.rename(columns = names, inplace=True)\n",
    "# wind['year'] = pd.to_datetime(wind['time']).map(lambda x: x.year)\n",
    "wind['month'] = pd.to_datetime(wind['time']).map(lambda x: x.month)\n",
    "wind['weekday'] = pd.to_datetime(wind['time']).map(lambda x: x.weekday())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind.to_csv('../data/wind_train.csv', index=True, index_label='datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config file for WanDB sweeps\n",
    "\n",
    "def make_config_file(file_path: str, df_len: int) -> Dict:\n",
    "    train_number: float = df_len * .7\n",
    "    validation_number: float = df_len *.9\n",
    "    config_default={\n",
    "      \"model_name\": \"DecoderTransformer\",\n",
    "      \"model_type\": \"PyTorch\",\n",
    "      \"takes_target\": False,\n",
    "      \"model_params\": {\n",
    "      \"n_time_series\":30,\n",
    "      \"n_head\": 8,\n",
    "      \"forecast_history\":90,\n",
    "      \"n_embd\": 1, \n",
    "      \"num_layer\": 5,\n",
    "      \"dropout\":0.1,\n",
    "      \"q_len\": 1,\n",
    "      \"scale_att\": False,\n",
    "      \"forecast_length\": 30, \n",
    "      \"additional_params\":{}\n",
    "     },\n",
    "     \"dataset_params\":\n",
    "     {\n",
    "         \"class\": \"default\",\n",
    "          \"training_path\": file_path,\n",
    "          \"validation_path\": file_path,\n",
    "          \"test_path\": file_path,\n",
    "          \"batch_size\":64,\n",
    "          \"forecast_history\":90,\n",
    "          \"forecast_length\":30,\n",
    "          \"train_end\": int(train_number),\n",
    "          \"valid_start\":int(train_number+1),\n",
    "          \"valid_end\": int(validation_number),\n",
    "          \"target_col\": ['Austria'],\n",
    "          \"relevant_cols\": ['Austria', 'Belgium', 'Bulgaria', 'Switzerland', 'Czechia',\n",
    "                            'Germany', 'Denmark', 'Estonia', 'Spain', 'Finland', 'France', 'Greece',\n",
    "                            'Croatia', 'Hungary', 'Ireland', 'Italy', 'Lithuania', 'Luxembourg',\n",
    "                            'Latvia', 'Netherlands', 'Norway', 'Poland', 'Portugal', 'Romania',\n",
    "                            'Slovenia', 'Slovakia', 'Sweden', 'United Kingdom', 'month', 'weekday'],\n",
    "          \"scaler\": \"StandardScaler\", \n",
    "          \"interpolate\": False,\n",
    "          \"sort_column\":\"time\",\n",
    "     },\n",
    "     \"training_params\":\n",
    "      {\n",
    "        \"criterion\":\"DilateLoss\",\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"optim_params\":\n",
    "        {\n",
    "        },\n",
    "        \"lr\": 0.001,\n",
    "        \"epochs\": 10,\n",
    "        \"batch_size\":64\n",
    "      },\n",
    "      \"early_stopping\": {\n",
    "          \"patience\":3\n",
    "      },\n",
    "      \"GCS\": False,\n",
    "      \"sweep\":False,\n",
    "      \"wandb\":False,\n",
    "      \"forward_params\":{},\n",
    "      \"metrics\":[\"DilateLoss\"],\n",
    "      \"inference_params\":\n",
    "        {     \n",
    "              \"datetime_start\":\"2010-01-01\",\n",
    "                \"hours_to_forecast\": 2000, \n",
    "                \"test_csv_path\":file_path,\n",
    "                \"decoder_params\":{\n",
    "                    \"decoder_function\": \"simple_decode\", \n",
    "                  \"unsqueeze_dim\": 1\n",
    "                },\n",
    "                \"dataset_params\":{\n",
    "                  \"file_path\": file_path,\n",
    "                  \"forecast_history\":90,\n",
    "                  \"forecast_length\":30,\n",
    "                  \"relevant_cols\": ['Austria', 'Belgium', 'Bulgaria', 'Switzerland', 'Czechia',\n",
    "                            'Germany', 'Denmark', 'Estonia', 'Spain', 'Finland', 'France', 'Greece',\n",
    "                            'Croatia', 'Hungary', 'Ireland', 'Italy', 'Lithuania', 'Luxembourg',\n",
    "                            'Latvia', 'Netherlands', 'Norway', 'Poland', 'Portugal', 'Romania',\n",
    "                            'Slovenia', 'Slovakia', 'Sweden', 'United Kingdom', 'month', 'weekday'],\n",
    "                  \"target_col\": ['Austria'],\n",
    "                  \"scaling\": \"StandardScaler\",\n",
    "                  \"interpolate_param\": False\n",
    "                }\n",
    "          },\n",
    "    }\n",
    "\n",
    "    return config_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path: str = '../data/wind_train.csv'\n",
    "full_len: int = len(pd.read_csv(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_file: Dict = make_config_file(file_path, full_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mloloheia\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.27<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">youthful-snowball-51</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/loloheia/pretrained-wind-updated\" target=\"_blank\">https://wandb.ai/loloheia/pretrained-wind-updated</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/loloheia/pretrained-wind-updated/runs/1h9f9imo\" target=\"_blank\">https://wandb.ai/loloheia/pretrained-wind-updated/runs/1h9f9imo</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\Lorenzo\\PycharmProjects\\flow-forecast\\notebooks\\wandb\\run-20210519_100911-1h9f9imo</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"pretrained-wind-updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interpolate should be below\n",
      "[]\n",
      "Now loading ../data/wind_train.csv\n",
      "scaling now\n",
      "interpolate should be below\n",
      "[]\n",
      "Now loading ../data/wind_train.csv\n",
      "scaling now\n",
      "interpolate should be below\n",
      "[]\n",
      "Now loading ../data/wind_train.csv\n",
      "scaling now\n",
      "Using Wandb config:\n",
      "{}\n",
      "Torch is using cuda\n",
      "running torch_single_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "Using non-full backward hooks on a Module that does not take as input a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The running loss is: \n",
      "783.2793531417847\n",
      "The number of items in train is: 118\n",
      "The loss for epoch 0\n",
      "6.637960619845633\n",
      "Computing validation loss\n",
      "running torch_single_train\n",
      "The running loss is: \n",
      "575.8933448791504\n",
      "The number of items in train is: 118\n",
      "The loss for epoch 1\n",
      "4.880452075247037\n",
      "Computing validation loss\n",
      "running torch_single_train\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-31a3e12656cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrained_model\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mPyTorchForecast\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"PyTorch\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmake_config_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\PycharmProjects\\flow-forecast\\flood_forecast\\trainer.py\u001b[0m in \u001b[0;36mtrain_function\u001b[1;34m(model_type, params)\u001b[0m\n\u001b[0;32m     63\u001b[0m                                 \u001b[0mtraining_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"training_params\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                                 \u001b[0mtakes_target\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtakes_target\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                                 forward_params={})\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;31m# To do delete\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"scaler\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\flow-forecast\\flood_forecast\\pytorch_training.py\u001b[0m in \u001b[0;36mtrain_transformer_style\u001b[1;34m(model, training_params, takes_target, forward_params, model_filepath)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[0mmeta_representation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[0mmeta_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m             multi_targets=num_targets)\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The loss for epoch \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\flow-forecast\\flood_forecast\\pytorch_training.py\u001b[0m in \u001b[0;36mtorch_single_train\u001b[1;34m(model, opt, criterion, data_loader, takes_target, meta_data_model, meta_data_model_representation, meta_loss, multi_targets, forward_params)\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[0moutput_std\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstddev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobablistic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_std\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmulti_targets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Warning: high loss detected\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\flow-forecast\\flood_forecast\\pytorch_training.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(labels, output, src, criterion, validation_dataset, probabilistic, output_std, m)\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\flow-forecast\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\flow-forecast\\flood_forecast\\custom\\dilate_loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, targets, outputs)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mloss_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftdtw_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mpath_dtw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPathDTWBatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath_dtw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mOmega\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mloss_temporal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mOmega\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mN_output\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mN_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\flow-forecast\\flood_forecast\\custom\\dilate_loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(ctx, D, gamma)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# loop over all D in the batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_cpu_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ_cpu_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mE_cpu_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtw_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD_cpu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m             \u001b[0mgrad_gpu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_cpu_k\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m             \u001b[0mQ_gpu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_cpu_k\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             \u001b[0mE_gpu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mE_cpu_k\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_model: PyTorchForecast = train_function(\"PyTorch\", make_config_file(file_path, full_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flood-forecast",
   "language": "python",
   "name": "flood-forecast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
